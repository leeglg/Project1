{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeglg/Project1/blob/master/4_evaluation/notebooks/lighteval_evaluate_and_analyse_your_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZVw9f5QYWPL"
      },
      "source": [
        "# lighteval is your AI evaluation library\n",
        "\n",
        "This notebook explores how you can use lighteval to evaluate and compare LLMs.\n",
        "\n",
        "`lighteval` has been around a while and it's a great tool for getting eval score on major benchmarks. It's just been refactored to support being used like a library in Python, which makes it great for comparing models across benchmarks.\n",
        "\n",
        "So let's dig in to some eval scores.\n",
        "\n",
        "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
        "    <h2 style='margin: 0;color:blue'>Exercise: Evaluate Your Own Model</h2>\n",
        "    <p>Now that you've seen how to evaluate models on specific domains, try evaluating a model on a domain that interests you.</p>\n",
        "    <p><b>Difficulty Levels</b></p>\n",
        "    <p>üê¢ Use the existing medical domain tasks but evaluate a different model from the Hugging Face hub</p>\n",
        "    <p>üêï Create a new domain evaluation by selecting different MMLU tasks (e.g., computer science, mathematics, physics)</p>\n",
        "    <p>ü¶Å Create a custom evaluation task using LightEval's task framework and evaluate models on your specific domain</p>\n",
        "</div>\n",
        "\n",
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afW_CLJoPCnF"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq -U \"torch<2.5\" \"torchvision<2.5\" \"torchaudio<2.5\" --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip list | grep torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xMnn_cQ1EEi"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq git+https://github.com/huggingface/lighteval.git tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDKs5ShvXw8K"
      },
      "source": [
        "## Setup `lighteval` evaluation\n",
        "\n",
        "We need to setup the evaluation environment and pipeline. Much of this we will disable because we're keeping things in the notebook, but we could also use `push_to_hub` or `push_to_tensorboard`.\n",
        "\n",
        "### `push_to_hub`\n",
        "\n",
        "This is useful if we're evaluating a model and want to persist its evaluation with weights and configuration on the Hugging Face hub.\n",
        "\n",
        "### `push_to_tensorboard`\n",
        "\n",
        "This would be useful if we were building an evaluation tool or script, where we wanted to view results within tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cUebd-z6IWs"
      },
      "outputs": [],
      "source": [
        "import lighteval\n",
        "import os\n",
        "from datetime import timedelta\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "from lighteval.logging.evaluation_tracker import EvaluationTracker\n",
        "from lighteval.logging.hierarchical_logger import hlog_warn, htrack\n",
        "from lighteval.models.model_config import create_model_config\n",
        "from lighteval.pipeline import EnvConfig, ParallelismManager, Pipeline, PipelineParameters\n",
        "\n",
        "TOKEN = os.getenv(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muikmXNQXgFv"
      },
      "outputs": [],
      "source": [
        "env_config = EnvConfig(token=TOKEN, cache_dir=\"~/tmp\")\n",
        "\n",
        "evaluation_tracker = EvaluationTracker(\n",
        "    output_dir=\"~/tmp\",\n",
        "    save_details=False,\n",
        "    push_to_hub=False,\n",
        "    push_to_tensorboard=False,\n",
        "    public=False,\n",
        "    hub_results_org=False,\n",
        ")\n",
        "\n",
        "pipeline_params = PipelineParameters(\n",
        "    launcher_type=ParallelismManager.ACCELERATE,\n",
        "    env_config=env_config,\n",
        "    job_id=1,\n",
        "    override_batch_size=1,\n",
        "    num_fewshot_seeds=0,\n",
        "    max_samples=10,\n",
        "    use_chat_template=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsNjwzCtltkA"
      },
      "source": [
        "# Compares models with `lighteval`\n",
        "\n",
        "We are going to compare two small LLMs on a domain. We will use `Qwen2.5-0.5B` and `SmolLM2-360M-Instruct` and we will evaluate them on a medical domain.\n",
        "\n",
        "We can create a domain evaluation from a subset of MMLU evaluations, by defining the evaluation tasks. In lighteval, tasks are described as strings.\n",
        "\n",
        "`{suite}|{task}:{subtask}|{num_few_shot}|{0 or 1 to reduce num_few_shot if prompt is too long}`\n",
        "\n",
        "Therefore, we will pass our list of medicine related tasks like this:\n",
        "\n",
        "```\n",
        "\"leaderboard|mmlu:anatomy|5|0,leaderboard|mmlu:professional_medicine|5|0,leaderboard|mmlu:high_school_biology|5|0,leaderboard|mmlu:high_school_chemistry|5|0\"\n",
        "```\n",
        "\n",
        "Which can be translated to :\n",
        "\n",
        "| Suite | Task | Num Fewshot Example | Limit Fewshots |\n",
        "|---|---|---|---|\n",
        "| leaderboard | mmlu:anatomy | 5 | False |\n",
        "| leaderboard | mmlu:professional_medicine | 5 | False |\n",
        "| leaderboard | mmlu:high_school_biology | 5 | False |\n",
        "| leaderboard | mmlu:high_school_chemistry | 5 | False |\n",
        "\n",
        "For a full list of lighteval supported tasks. Checkout this page in [the documentation](https://github.com/huggingface/lighteval/wiki/Available-Tasks)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTqsizv9mVbO"
      },
      "outputs": [],
      "source": [
        "domain_tasks = \"leaderboard|mmlu:anatomy|5|0,leaderboard|mmlu:professional_medicine|5|0,leaderboard|mmlu:high_school_biology|5|0,leaderboard|mmlu:high_school_chemistry|5|0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwcJklSFX4H6"
      },
      "source": [
        "# Evaluate Qwen2.5 0.5B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXJuaXVxUNBO"
      },
      "outputs": [],
      "source": [
        "qwen_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    tasks=domain_tasks,\n",
        "    pipeline_parameters=pipeline_params,\n",
        "    evaluation_tracker=evaluation_tracker,\n",
        "    model=qwen_model\n",
        ")\n",
        "\n",
        "pipeline.evaluate()\n",
        "\n",
        "qwen_results = pipeline.get_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIwCaCxJX_hA"
      },
      "source": [
        "# Evaluate SmolLM 360M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxg0RtlNVT4y"
      },
      "outputs": [],
      "source": [
        "smol_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    tasks=domain_tasks,\n",
        "    pipeline_parameters=pipeline_params,\n",
        "    evaluation_tracker=evaluation_tracker,\n",
        "    model=smol_model\n",
        ")\n",
        "\n",
        "pipeline.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdjyzfKHVt52"
      },
      "outputs": [],
      "source": [
        "smol_results = pipeline.get_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eugvMFfgV1VD"
      },
      "outputs": [],
      "source": [
        "pipeline.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HD8aFwSYGHu"
      },
      "source": [
        "# Visualize Results\n",
        "\n",
        "Now that we have results from the two models we can visualize them side-by-side. We'll keep visualisation simple here, but with this data structure you could represent scores in many ways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sReqrgQUO9r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame.from_records(smol_results[\"results\"]).T[\"acc\"].rename(\"SmolLM2-360M-Instruct\")\n",
        "_df = pd.DataFrame.from_records(qwen_results[\"results\"]).T[\"acc\"].rename(\"Qwen2-0.5B-DPO\")\n",
        "df = pd.concat([df, _df], axis=1)\n",
        "df.plot(kind=\"barh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJEbQeYDplKX"
      },
      "source": [
        "# üíê That's it!\n",
        "\n",
        "You have a handy notebook to view model evals. You could use this to:\n",
        "\n",
        "- select the right model for your inference use case\n",
        "- evaluate checkpoints during training\n",
        "- share model scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWdS38syaipm"
      },
      "source": [
        "üèÉNext Steps\n",
        "\n",
        "- If you want to go deeper into your evaluation results check out this [notebook](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/examples/comparing_task_formulations.ipynb)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}